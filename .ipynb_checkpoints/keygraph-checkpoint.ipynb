{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyvis\n",
      "  Using cached pyvis-0.3.2-py3-none-any.whl (756 kB)\n",
      "Requirement already satisfied: ipython>=5.3.0 in c:\\users\\acer\\anaconda3\\envs\\ml\\lib\\site-packages (from pyvis) (7.28.0)\n",
      "Requirement already satisfied: jinja2>=2.9.6 in c:\\users\\acer\\anaconda3\\envs\\ml\\lib\\site-packages (from pyvis) (3.0.2)\n",
      "Requirement already satisfied: networkx>=1.11 in c:\\users\\acer\\anaconda3\\envs\\ml\\lib\\site-packages (from pyvis) (3.1)\n",
      "Collecting jsonpickle>=1.4.1\n",
      "  Using cached jsonpickle-3.0.1-py2.py3-none-any.whl (40 kB)\n",
      "Requirement already satisfied: setuptools>=18.5 in c:\\users\\acer\\anaconda3\\envs\\ml\\lib\\site-packages (from ipython>=5.3.0->pyvis) (58.0.4)\n",
      "Requirement already satisfied: decorator in c:\\users\\acer\\anaconda3\\envs\\ml\\lib\\site-packages (from ipython>=5.3.0->pyvis) (5.1.0)\n",
      "Requirement already satisfied: backcall in c:\\users\\acer\\anaconda3\\envs\\ml\\lib\\site-packages (from ipython>=5.3.0->pyvis) (0.2.0)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\acer\\anaconda3\\envs\\ml\\lib\\site-packages (from ipython>=5.3.0->pyvis) (0.18.0)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in c:\\users\\acer\\anaconda3\\envs\\ml\\lib\\site-packages (from ipython>=5.3.0->pyvis) (3.0.20)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\acer\\anaconda3\\envs\\ml\\lib\\site-packages (from ipython>=5.3.0->pyvis) (0.1.3)\n",
      "Requirement already satisfied: traitlets>=4.2 in c:\\users\\acer\\anaconda3\\envs\\ml\\lib\\site-packages (from ipython>=5.3.0->pyvis) (5.1.0)\n",
      "Requirement already satisfied: pygments in c:\\users\\acer\\anaconda3\\envs\\ml\\lib\\site-packages (from ipython>=5.3.0->pyvis) (2.10.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\acer\\anaconda3\\envs\\ml\\lib\\site-packages (from ipython>=5.3.0->pyvis) (0.4.4)\n",
      "Requirement already satisfied: pickleshare in c:\\users\\acer\\anaconda3\\envs\\ml\\lib\\site-packages (from ipython>=5.3.0->pyvis) (0.7.5)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in c:\\users\\acer\\anaconda3\\envs\\ml\\lib\\site-packages (from jedi>=0.16->ipython>=5.3.0->pyvis) (0.8.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\acer\\anaconda3\\envs\\ml\\lib\\site-packages (from jinja2>=2.9.6->pyvis) (2.0.1)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\acer\\anaconda3\\envs\\ml\\lib\\site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=5.3.0->pyvis) (0.2.5)\n",
      "Installing collected packages: jsonpickle, pyvis\n",
      "Successfully installed jsonpickle-3.0.1 pyvis-0.3.2\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "!pip install pyvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import codecs\n",
    "import pprint\n",
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from networkx.algorithms.community.centrality import girvan_newman\n",
    "from networkx.algorithms.community.quality import modularity\n",
    "from pyvis.network import Network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from document import Document\n",
    " \n",
    "# sys.stdout = codecs.getwriter('utf_8')(sys.stdout)\n",
    "# sys.stdin = codecs.getreader('utf_8')(sys.stdin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Util:\n",
    "    @staticmethod\n",
    "    # Pretty-print a Python object\n",
    "    def pp(obj):\n",
    "        pp = pprint.PrettyPrinter(indent=4, width=160)\n",
    "        s = pp.pformat(obj)\n",
    "        return s    \n",
    "  \n",
    "    @staticmethod\n",
    "    # Read file name from the console\n",
    "    def get_file_name():\n",
    "        if (len(sys.argv) != 2):\n",
    "            print(\"Usage: #python %s file-name\" % sys.argv[0])\n",
    "            sys.exit()\n",
    "        return sys.argv[1]\n",
    " \n",
    "class KeyGraph:\n",
    "    def __init__(self, document, M=30, K=12):\n",
    "        self.document = document\n",
    "        self.base = self.compute_base(M)\n",
    "        self.G_C = self.compute_hubs(K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Compute base of frequently co-occurring words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def compute_base(self, M):\n",
    "        mtime0 = time.time()\n",
    "        print(\"Compute base: top %d words and edges\" % M)\n",
    "        # Sort words by their frequency (in ascending order)\n",
    "        freq_count = self.document.freq_count()\n",
    "        words_freq = sorted(freq_count.items(), key=lambda x: x[1])\n",
    "        \n",
    "        # Compute unique words        \n",
    "        self.words = [w for w, f in words_freq]\n",
    "        print(\"Unique words:\", len(self.words))\n",
    "        \n",
    "        # Calculate word frequency in sentences\n",
    "        self.wfs = self.calculate_wfs()\n",
    "        \n",
    "        # Determine high frequency words\n",
    "        # Include all words with the higher or same frequency than the Mth word\n",
    "        wf_min = words_freq[-M][1] if len(words_freq) > M else 0\n",
    "        hf = [w for w, f in words_freq if f >= wf_min]\n",
    "\n",
    "        # Adjust M to the number of high frequency words\n",
    "        M = len(hf)\n",
    "        print(\"Adjust M to\", M)\n",
    "        print(\"High frequency words:\", len(hf))\n",
    "\n",
    "        # Calculate co-occurrence degree of high-frequency words\n",
    "        co = self.calculate_co_occurrence(hf)\n",
    "\n",
    "        # Keep only the tightest links\n",
    "        # Include all links with the higher of same co-occurrence degree as the Mth link\n",
    "        c_min = co[-M][2] if len(co) > M else 0\n",
    "        co = [[i, j] for i, j, c in co if c >= c_min]\n",
    "        print(Util.pp(co))\n",
    "        mtime1 = time.time()\n",
    "        print(\"Execution time of compute base before find clusters: %.4f seconds\" % (mtime1 - mtime0))\n",
    "\n",
    "        # Compute the clusters (which are the basis for islands)\n",
    "        self.find_clusters(co)\n",
    "        mtime2 = time.time()\n",
    "        print(\"Execution time for find clusters: %4f\" % (mtime2 - mtime1))\n",
    "        return co\n",
    "    \n",
    "#   Calculate word frequency in sentences\n",
    "    def calculate_wfs(self):\n",
    "        wfs = {}\n",
    "        for w in self.words:\n",
    "            for s_idx, s in enumerate(self.document.sentences):\n",
    "                if w not in wfs:\n",
    "                    wfs[w] = {}\n",
    "                wfs[w][s_idx] = s.count(w)\n",
    "        return wfs\n",
    "    \n",
    "#   Calculate co-occurrence degree of high-frequency words\n",
    "    def calculate_co_occurrence(self, hf):\n",
    "        co = {}\n",
    "        for hf1 in hf:\n",
    "            co[hf1] = {}\n",
    "            for hf2 in hf[hf.index(hf1)+1:]:\n",
    "                co[hf1][hf2] = 0\n",
    "                for s in self.document.sentences:\n",
    "                    # Why sum products, not min, as in Ohsawa (1998)?\n",
    "                    # co[hf1][hf2] += s.count(hf1) * s.count(hf2)\n",
    "                    co[hf1][hf2] += min(s.count(hf1), s.count(hf2))\n",
    "        co_list = []\n",
    "        for x in co.keys():\n",
    "            for y in co[x].keys():\n",
    "                co_list.append([x, y, co[x][y]])\n",
    "        co_list.sort(key=lambda a: a[2])\n",
    "        return co_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Detect communities in the base<br>\n",
    "  The base is a list of pairs of words that are co-occurring in the document<br>\n",
    "  Clusters will be used to define islands of connected words, however, the edges<br>\n",
    "  between the clusters do not be removed to do that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (Temp/ipykernel_6396/463970957.py, line 247)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\Acer\\AppData\\Local\\Temp/ipykernel_6396/463970957.py\"\u001b[1;36m, line \u001b[1;32m247\u001b[0m\n\u001b[1;33m    fname = Util.get_file_name()\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "    def find_clusters(self, base):\n",
    "        G = nx.Graph()\n",
    "        for i, j in base:\n",
    "            G.add_edge(i, j)\n",
    "        \n",
    "        communities = girvan_newman(G)\n",
    "        communities_by_quality = [(c, modularity(G, c)) for c in communities]\n",
    "        c_best = sorted([(c, m) for c, m in communities_by_quality], key=lambda x: x[1], reverse=True)\n",
    "        c_best = c_best[0][0]\n",
    "        # print(Util.pp(communities_by_quality))\n",
    "        print(\"Clusters:\", modularity(G, c_best), c_best)\n",
    "        \n",
    "        # only include clusters of more than one node (for now)\n",
    "        # self.clusters = [c for c in c_best if len(c) > 1]\n",
    "\n",
    "        # Include all clusters (do not remove edges between clusters)\n",
    "        self.clusters = c_best\n",
    "\n",
    "        # for cluster in c_best:\n",
    "        #     print(G.subgraph(cluster).edges())\n",
    "        self.new_base = [edge for cluster in c_best for edge in G.subgraph(cluster).edges()]\n",
    "        # return new_base\n",
    "\n",
    "        # Links between clusters could be shown in a different color or dotted line\n",
    "    def compute_hubs(self, K):\n",
    "        print(\"Compute hubs: top %d key terms and bridges\" % K)\n",
    "        # Extract nodes in the base\n",
    "        G_base = set([x for pair in self.base for x in pair])\n",
    "\n",
    "        # Remove high frequency words from G_base, leaving non-high frequency words\n",
    "        self.words = [w for w in self.words if w not in G_base]\n",
    "        print(\"Non-high frequency words:\", len(self.words))\n",
    "\n",
    "        # Compute key terms that connect clusters\n",
    "        key = self.key(self.words)\n",
    "        print(\"Key terms:\", Util.pp(key))\n",
    "\n",
    "        # Sort terms in D by keys\n",
    "        # Include all words with the higher or same frequency than the Kth word\n",
    "        high_key = sorted(key.items(), key=lambda x: x[1])\n",
    "        k_min = high_key[-K][1] if len(high_key) > K else 0\n",
    "        high_key = [w for w, k in high_key if k >= k_min]\n",
    "\n",
    "        # Adjust K to the number of high key words\n",
    "        K = len(high_key)\n",
    "        \n",
    "        print(\"Adjusted K:\", K)\n",
    "        print(Util.pp(high_key))\n",
    " \n",
    "        # Calculate columns\n",
    "        C = self.columns(high_key, G_base)\n",
    "        \n",
    "        print(Util.pp(C))\n",
    "\n",
    "        # Compute the top links between key terms (red nodes) and columns\n",
    "        # Include all links with the higher of same co-occurrence degree as the Kth link\n",
    "        c_min = C[-K][2] if len(C) > K else 0\n",
    "        G_C = [[i, j] for i, j, c in C if c >= c_min]\n",
    "\n",
    "        # Compute adjacency list\n",
    "        self.base_adj = self.adjacency_list(self.base, G_C)\n",
    "        \n",
    "        return G_C\n",
    "        \n",
    "    # Compute key terms that connect clusters\n",
    "    def key(self, words):\n",
    "        # optimization: compute the neighbors of all clusters ahead of time\n",
    "        neighbors = {}\n",
    "        for g_idx, g in enumerate(self.clusters):\n",
    "            neighbors[g_idx] = self.neighbors(g)\n",
    "        # key is a dictionary of the formã€€key = {w: key value}\n",
    "        key = {}\n",
    "        for w in words:\n",
    "            # print(\"keyword: {}\".format(w))\n",
    "            product = 1.0\n",
    "            for g_idx, g in enumerate(self.clusters):\n",
    "                # print(\"g\", g_)\n",
    "                # print(\"neighbors\", neighbors)\n",
    "                based = self.based(w, g)\n",
    "                # print(\"based\", based)\n",
    "                product *= 1 - based/neighbors[g_idx]\n",
    "            key[w] = 1.0 - product\n",
    "        return key\n",
    "\n",
    "    # Count of words in sentences including words in cluster g \n",
    "    def neighbors(self, g):\n",
    "        neighbors = 0\n",
    "        for s, sentence in enumerate(self.document.sentences):\n",
    "            g_s = 0\n",
    "            for t in g:\n",
    "                g_s += self.wfs[t][s]\n",
    "            # print(\"g_s\", g_s)\n",
    "            for w in sentence:\n",
    "                # print(s, w)\n",
    "                w_s = self.wfs[w][s]\n",
    "                if w in g:\n",
    "                    # print(\"w in g\")\n",
    "                    neighbors += + w_s * (g_s - w_s)\n",
    "                else:\n",
    "                    # print(\"w not in g\")\n",
    "                    neighbors += w_s * g_s\n",
    "        return neighbors\n",
    "        \n",
    "    # Count how many times w appeared in D based on concept represented by cluster g\n",
    "    def based(self, w, g):\n",
    "        based = 0\n",
    "        for s, sentence in enumerate(self.document.sentences):\n",
    "            # print(s, w)\n",
    "            g_s = 0\n",
    "            for t in g:\n",
    "                g_s += self.wfs[t][s]\n",
    "            # print(\"g_s\", g_s)\n",
    "            w_s = self.wfs[w][s]\n",
    "            if w in g:\n",
    "                # print(\"w in g\")\n",
    "                based += w_s * (g_s - w_s)\n",
    "            else:\n",
    "                # print(\"w not in g\")\n",
    "                based += w_s * g_s\n",
    "        return based\n",
    "    \n",
    "    # Calculate columns c(wi,wj)\n",
    "    def columns(self, hk, base):\n",
    "        c = {}\n",
    "        for k in hk:\n",
    "            c[k] = {}\n",
    "            for b in base:\n",
    "                c[k][b] = 0\n",
    "                for s in self.document.sentences:\n",
    "                    c[k][b] += min(s.count(k), s.count(b))\n",
    "        n_clusters = self.clusters_touching(c)\n",
    "        print(Util.pp(n_clusters))\n",
    "        c_list = [] \n",
    "        for k in c.keys():\n",
    "            for b in c[k].keys():\n",
    "                if n_clusters[k] > 1 and c[k][b] > 0:\n",
    "                    c_list.append([k, b, c[k][b]])\n",
    "        c_list.sort(key=lambda a: a[2])\n",
    "        return c_list \n",
    "\n",
    "    # How many clusters does each column touch\n",
    "    def clusters_touching(self, c):\n",
    "        n_clusters = {}\n",
    "        for k in c.keys():\n",
    "            # print(\"k\", k)\n",
    "            n_clusters[k] = 0\n",
    "            for g in self.clusters:\n",
    "                # print(\"g\", g)\n",
    "                in_cluster = 0\n",
    "                for b in c[k].keys():\n",
    "                    # print(\"b\", b)\n",
    "                    if c[k][b] > 0 and b in g:\n",
    "                        # print(\"b in g\")\n",
    "                        in_cluster = 1\n",
    "                n_clusters[k] += in_cluster\n",
    "        return n_clusters\n",
    "    \n",
    "    # Create an adjacency list\n",
    "    def adjacency_list(self, base, G_C):\n",
    "        a_list = {}\n",
    "        \n",
    "        for i, j in base:\n",
    "            if i in a_list:\n",
    "                a_list[i].append([j,'base'])\n",
    "            else:\n",
    "                a_list[i] = [[j,'base']]\n",
    "            if j in a_list:\n",
    "                a_list[j].append([i,'base'])\n",
    "            else:\n",
    "                a_list[j] = [[i,'base']]\n",
    "        \n",
    "        for i, j in G_C:\n",
    "            if i in a_list:\n",
    "                a_list[i].append([j,'key'])\n",
    "            else:\n",
    "                a_list[i] = [[j,'key']]\n",
    "            if j in a_list:\n",
    "                a_list[j].append([i,'key'])\n",
    "            else:\n",
    "                a_list[j] = [[i,'key']]\n",
    "        \n",
    "        return a_list\n",
    "    \n",
    "    def save_adjacency_list(self, fname):\n",
    "        fout = codecs.open(\"./adjacency_list/\" + fname + \".txt\", \"w\", \"utf-8\")\n",
    "        fout.write(Util.pp(self.base_adj))\n",
    "        fout.close()\n",
    "    def draw(self, fname):\n",
    "        G = nx.Graph()\n",
    "        \n",
    "        # Add all nodes in clusters\n",
    "        for cluster in self.clusters:\n",
    "            G.add_nodes_from(cluster, color='black')\n",
    "\n",
    "        # Add edges for nodes in clusters\n",
    "        for i, j in self.base:\n",
    "            if (i, j) in self.new_base:\n",
    "                G.add_edge(i, j)\n",
    "        \n",
    "        # Add edges for nodes in key terms\n",
    "        for i, j in self.G_C:\n",
    "            G.add_node(i, color='red')\n",
    "            G.add_edge(i, j, color='red')\n",
    "\n",
    "        # Remove isolated nodes\n",
    "        G.remove_nodes_from(list(nx.isolates(G)))\n",
    "        network = Network('600px', '600px')\n",
    "        network.from_nx(G)\n",
    "        network.show(\"./graphs/{}.html\".format(fname))\n",
    "    \n",
    "    # Draw keygraph in dot format\n",
    "    def draw_dot(self, fname):\n",
    "        fout = codecs.open(\"./dot/\" + fname + \".dot\",\"w\",\"utf-8\")\n",
    "        fout.write('graph keygraph {\\n')\n",
    "        fout.write('graph [size=\"10,10\", overlap=\"scale\"]\\n')\n",
    "    \n",
    "        g = []\n",
    "        for i, j in self.base:\n",
    "            g.append(i)\n",
    "            g.append(j)\n",
    "        for i in set(g):\n",
    "            fout.write(self.quote(i) + ' [color=\"black\"]\\n')\n",
    "        k = []\n",
    "        for i, j in self.G_C:\n",
    "            k.append(i)\n",
    "        for i in set(k):\n",
    "            fout.write(self.quote(i) + ' [color=\"red\"]\\n')\n",
    "            \n",
    "        for i, j in self.base:\n",
    "            fout.write(self.quote(i) + '--' + self.quote(j) +'\\n')\n",
    "        for i, j in self.G_C:\n",
    "            fout.write(self.quote(i) + '--' + self.quote(j) + ' [color=\"red\", style=\"dotted\"]\\n')\n",
    "        fout.write('}')\n",
    "        fout.close()\n",
    "        \n",
    "    # Add optional quotes around a name\n",
    "    def quote(self, name):\n",
    "        if 1 in [c in name for c in ['-', '/', '.', '\\'']] or name in [\"graph\"]:\n",
    "            return \"\\\"{}\\\"\".format(name)\n",
    "        return name\n",
    "             \n",
    "#-----------Main----------------\n",
    "if __name__ == \"__main__\":\n",
    "        stime = time.time() \n",
    "\n",
    "    \n",
    "#   Create a document\n",
    "    fname = Util.get_file_name()\n",
    "    doc = Document(file_name = 'txt_files/' + fname + '.txt')\n",
    "        \n",
    "#   Create a keygraph\n",
    "    kg = KeyGraph(doc, M=20, K=8) # default: M=30, K=12\n",
    "    print(\"clusters\", kg.clusters)\n",
    "    kg.save_adjacency_list(fname)\n",
    "    mtime = time.time()\n",
    "    kg.draw(fname)\n",
    "    print(\"Time to draw keygraph: %.4f\", (mtime - stime))\n",
    "    \n",
    "    etime = time.time()\n",
    "    print(\"Execution time: %.4f seconds\" % (etime - stime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
